\documentclass{acm_proc_article-sp}
\usepackage{times}
\usepackage{url}
\usepackage{graphics}
\usepackage{color}
\newcommand{\want}[1]{}
\newcommand{\idea}[1]{}
\newcommand{\note}[1]{}

% Mark up a point that we want to flesh out into more text
\newcommand{\x}[1]{{\color{blue} #1}\\}

% Mark up some work that we need to do in order for this paper to be telling the truth
\newcommand{\todo}[1]{{\color{red} #1}\\}

\newcommand{\maybe}[1]{}

\begin{document}

\toappear

\bibliographystyle{plain}

\title{What is disputed on the web?}

%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

\numberofauthors{5}

\author{
\alignauthor Rob Ennals\\
       \affaddr{Intel Labs Berkeley}\\
       \affaddr{2150 Shattuck Ave}\\
       \affaddr{Berkeley, CA, USA}\\
       \email{robert.ennals@intel.com}
\alignauthor Dan Byler\\
       \affaddr{School of Information}\\
       \affaddr{University of California at Berkeley}\\
       \affaddr{Berkeley, CA, USA}\\
       \email{daniel.byler@berkeley.edu}
\alignauthor John Mark Agosta\\
       \affaddr{Intel Labs Santa Clara}\\
       \affaddr{2200 Mission College Blvd}\\
       \affaddr{Santa Clara, CA, USA}\\
       \email{john.m.agosta@intel.com}
\and
\alignauthor Barbara Rosario\\
       \affaddr{Intel Labs Santa Clara}\\
       \affaddr{2200 Mission College Blvd}\\
       \affaddr{Santa Clara, CA, USA}\\
       \email{john.m.agosta@intel.com}
\alignauthor Tye Rattenbury\\
       \affaddr{Intel Principles and Practices}\\
       \affaddr{2200 Mission College Blvd}\\
       \affaddr{Santa Clara, CA, USA}\\
       \email{tye.l.rattenbury@intel.com}
}

%\sloppy


\maketitle

%RULE: Don't cite media reports unless I have to - some reviewers don't like it


\abstract

We describe a method for the automatic acquisition of a corpus of disputed claims from the web. We consider a claim to be disputed if a document on the web suggests that this claim is wrong and that other people on the web are suggesting that it is true.

Our tool extracts disputed claims by searching the web for templates such as ``falsely claimed that X'' or ``the myth that X'', and then analysing the parsed sentence structure to identify claims that seem to have the correct form. Given a set of known disputed claims, we can infer further templates by searching for other contexts in which known disputed claims appear.

We believe that our corpus of disputed claims could be useful for a wide range of applications related to information credibility on the web. 

%\subsection{Categories and Subject Descriptors}
%\todo{Categories, terms, and keywords need updating}
\category{H.3.1}{INFORMATION STORAGE AND RETRIEVAL}{Content Analysis and Indexing}
\category{I.2.7}{ARTIFICIAL INTELLIGENCE}{Natural Language Processing}

\terms{Design, Human Factors}

\keywords{Sensemaking, Annotation, Argumentation, Web, CSCW}

\tolerance=400 
  % makes some lines with lots of white space, but 	
  % tends to prevent words from sticking out in the margin

\section{Introduction}

The web contains a vast number of documents written by a vast number of people. In many cases, these people disagree with each other, and the documents they write contain conflicting information. For users to extract reliable information from the web, it is useful for them to be able to determine when different documents disagree about a topic.

In this paper we describe a method for automatically acquiring a corpus of disputed claims from the web. We consider a claim to be {\it disputed} if some people believe both that others are making the claim, and that the claim is not true.  

We identify disputed claims by searching for lexical patterns such as ``the misconception that $S$''. For example, if a document contains the text ``the minconception that {\it the moon is made of cheese}'', this suggests that the author of the document believes both that the statement ``{\it the moon is made of cheese}'' is false, and that there are other authors who claim that ``{\it the moon is made of cheese}'' is true.

We have found many other patterns that can be used in this way, some of which we list in Figure~\ref{templates}. Pattern searching methods like these have been used by other authors previously, most notably by Marti Hearst~\cite{hearst-hyponyms} who looked for patterns such as ``{\it animals} such as {\it cats}'' to identify hyponyms.

\todo{TODO: we need to use template mining to get a more complete list of the templates that are good.}

Given a corpus of disputed claims $S$, we can identify other patterns that suggest that a claim is disputed by searching the web for contexts in which known disputed claims appear. This is an instance of the distributional hypothesis~\cite{distributional-hypothesis}, which theorises that if a pattern occurs with similar phrases filling its slots (it is distributionally similar) then it has a similar meaning.

We believe that this corpus will be useful for many purposes. We are currently using it as part of our Dispute Finder~\cite{www-disputefinder} project to automatically highlight phrases on web pages that are disputed by other sources. We are also exploring other applications of this corpus, including visualization tools that let users see what is disputed about topics they are interested in, or what has been disputed at particular times.

We are making our corpus publicly available for other researchers to download. We plan to continue to update our corpus as new pages are added to the web, and to develop tools that allow people to query it in useful ways.

We believe that ours is the first attempt to automatically acquire a corpus of disputed claims.

\todo{Need to put our data online somewhere so that others can download it.}

In this paper, we describe in more detail the way that we built this corpus, we explain ways that we think this corpus could be used, and we describe some features that we found in our data.

\x{Include graph showing process by which we import claims.}


\section{Background and Related Work}

\x{Information credibility is important.}
\x{People are increasingly relying on the web for information.}
\x{People read a wider range of sources that before, many of which they do not know whether to trust.}

Information credibility on the web is becoming increasingly important. In the past people typically obtained information from a relatively small set of sources such as books, TV channels, and radio stations. Most of these sources had a known reputation, and had publishing barriers and quality control to ensure that the information they provided met the standards expected from that source. The web is very different. A user has access to a vast number of different sources, but a user will know the reputation of only a small subset of these. Moreover, the internet has little in the way of publishing barriers or quality control. If users are to extract trustworthy information from the web, they need to either restrict themselves to a small set of trusted sources, or use some kind of mechanism to determine the credibility of the information that they access.

\x{Cite Pew study on this? Cite other people talking about misinformation}.

Given a document that contains information which may or may not be trustworthy, there are several ways that a reader can determine whether they should trust the information. They can check the reputation of the source; they can check whether it looks like a reliable document; or they can check whether the information in the document is consistent with information available from other sources.

To check the reputation of a source, a user can use a web site such as SourceWatch.org, which publishes manually curated information about the reputation and known biases of various sources. Trustpilot.com produce a Firefox extension that warns a user when they are looking at a web page hosted by a company that they believe is not trustworthy. Alternatively, a user can simply use a search engine such as Google to look for information about the source. While these tools can be very useful, trustworthy sources sometimes publish unreliable information. For example, a source may have been misled by an unreliable source they were using themselves. Moreover, a user may have a document from a source that is not known to be reliable, and still wish to know if the information itself is trustworthy.

Researchers have identified a variety of metrics that can be used to automatically estimate the quality of a document based on looking at its content. For Wikipedia, Blumenstock~\cite{Blumenstock2008} estimates the quality of an article by the word count and WikiTrust~\cite{wikitrust} identifies potentially unreliable sections of an article by analyzing its edit history. Custard and Sumner~\cite{Custard2005} use a combination of metrics to measure web site quality, including number of links and whether it contains videos. Fogg et al~\cite{Fogg} have shown that users commonly evaluate the credibility of a web site based on factors such as the design look, the information structure of the site, and the tone of the writing. Some of the factors identified by Fogg et al could potentially be measured automatically and used to guide a user.

In our research, we are following the third approach. We aim to inform users when information that they encounter is disputed by another source. We have built an extension to the Firefox web browser called Dispute Finder~\cite{www-disputefinder} that informs users when a web page that they are reading makes a claim that it knows to be disputed. For example, if a user is reading a page that says ``Elvis is Alive'' then Dispute Finder will highlight that statement as being disputed and direct the user to other sources that put forward alternative points of view. In its currently released version, Dispute Finder builds a corpus of disputed claims by allowing users to enter disputed claims manually, and scraping a small set of web sites that manually curate such claims; however it is difficult to make this approach scale to the huge number of claims on the web that are disputed.

Our primary motivation for automatically acquiring a large corpus of disputed claims is to use this corpus to enable tools like Dispute Finder to automatically inform users when they encounter information that this corpus says is disputed.

The problem of detecting disputed claims is closely related to the well-studied problem of detecting contradictions. A claim $H$ is {\it contradicted} if a document somewhere on the web makes a claim that implies that $H$ cannot be true. For example, the claim ``The moon is made of rock'' {\it contradicts} the claim ``The moon is made of cheese''.
A claim $H$ is {\it disputed} if a document somewhere on the web suggests both that some people are claiming $H$ and that those people are wrong. For example the text ``we disagree with the assertion that the moon is made of cheese'' {\it disputes the claim} ``the moon is made of cheese''. A {\it contradiction} is logical, while a {\it dispute} is social. In order for a claim to be disputed, the authors of the documents must be aware that there is a contradiction between their beliefs.

There has been significant work on detecting contradictions. Condoravdi et al~\cite{Condoravdi2003} argue that contradiction detection is one of the key tasks in language understanding. de Marneffe et al~\cite{deMarneffe2008} present a taxonomy of the different ways that claims can contradict each other and describe a system that combines many techniques to detect different kinds of contradiction. AuContraire~\cite{Ritter2008} uses TextRunner~\cite{textrunner} to infer subject-verb-object relationships, looks for cases where a verb maps the same subject to multiple objects, and uses semantic knowledge to determine when this implies that there is a contradiction. Harabagiu et al~\cite{Harabagiu2006} look for contradictions where one claim is a negated paraphrase of another. The RTE-3 Recognizing Textual Entailment challenge~\cite{rte-3} included an optional contradiction detection task, allowing different groups building contradiction detection algorithms to compare their results. 

Detecting contradictions has proven to be a hard problem. Much of the difficulty comes from the fact that it typically requires deep semantic knowledge to determine whether two statements that look like they might be contradictory actually are. ``George Bush is married to Barbara Bush'' does not contradict ``George Bush is married to Laura Bush'' because there is more than one George Bush. ``Alan Turing was born in England'' does not contradict ``Alan Turing was born in London'' because London is in England. ``It is raining is San Francisco'' does not contradict ``It is not raining in San Francisco'' if the statements were made at different times. 

We detect disputes rather than contradictions. Rather than looking for claims that contradict each other, we look for evidence that people believe that there is a contradiction and that this contradiction is important. The advantage of looking for disputes rather than contradictions is that is is much easier - humans do the hard work of identifying contradictions and deciding whether they are important. The disadvantage of looking for disputes is that this method will only identify contradictions that humans have already identified. 

\todo{Want some kind of numerical estimate of how many disputed claims we have. Maybe measure this by pair of unique nouns and verbs. Noun + verb or noun + noun = disputed claim?}

\maybe{Want to apply wikify to get nouns, and then use Chi-style category rules to find out what categories are disputed on the web?}

\input{relatedwork.tex}


\section{Finding Disputed Claims}

\x{Include a diagram showing the stages of our dispute finding process}
\maybe{Talk about theories of contrast???}
\maybe{Look at Kehler 2002 on CONTRAST???}

Figure~\ref{not-yet} shows the process we use to build our corpus of disputed claims. 
We find disputed claims by searching the web for for patterns like ``the misconception that $S$'' and ``it is not the case that $S$'' (Section~\ref{searchpattern}). 
For every page that contains the pattern, we grab the raw text from the end of the prefix string, up until the end end of the sentence. We then pass each such block of text to a parser that discards any text that does not appear to be a statement (Section~\ref{filterclaim}).

Given a set of known disputed claims, we then go back and generate new patterns by searching for those claims on the web and looking for contexts in which those claims often appear.


\subsection{Searching for Patterns}
\label{searchpattern}

We search the web for patterns that indicate that a claim is disputed. Figure~\ref{templates} shows a subset of the patterns that we search for. For each pattern, we show the number of times that Google estimates that the pattern appears on the web. 

Rather than building our own search engine, we use the Yahoo BOSS search API~\cite{yahoo-boss} to search for patterns. We search Yahoo BOSS for a raw string such as ``the misconception that'' and then download every web page that Yahoo lists in its search results.

We form a {\it candidate claim} by taking all text from the end of the prefix we searched for, up to the nearest sentence-ending punctuation character or HTML tag (e.g. '.','?','!',\texttt{</p>},\texttt{</h1>}, etc). 

We want to be able to incrementally update our corpus as new pages appear on the web, without repeatedly downloading the same pages. The Yahoo BOSS API does not offer the ability to request only pages updated in a given date range. We thus simulate this behaviour by instead including a literal date string in our queries. For example, if we want to find claims that were disputed on January 4th 2010 then rather than searching for \texttt{``it is not true that''}, we instead search for \texttt{``it is not true that'' ``January 4 2010''}.

The reasoning behind this technique is that many articles on the web include the date the article was posted in the text of the web page, particularly news articles and blog posts. This approach has false positives and false negatives. Some web pages include dates that are not the date the article was posted, and some web pages either don't include the date the article was posted, or write their date in a different format. Indeed our current implementation suffers from a systematic bias towards sites that write their dates in US format (month first) rather than UK format (day first).

\todo{Produce some kind of measure of how well date searching actually works}

Another advantage of including the date in the search query is it reduces the impact of Yahoo's limit of only 1000 results per query. Since we include the date in the query, this translates into a limit of only 1000 results per pattern per day, which gives us access to a much larger set of pages.

Since we are using an existing search engine, we are limited to searching for simple text strings. This leads us to generate multiple patterns that are almost the same, other than for synonyms. E.g. ``the lie that'' vs ``the deceit that''. We are also restricted to searching for patterns that consist entirely of either a prefix or a suffix. For example we could not search for ``say that $S$ despite'', because there is no way to require that ``despite'' and ``say that'' be in the same sentence. While we could do the search anyway, and then filter out all pages in which the parts were not in the same sentence, this would be inefficient, since many pages would not contain the pattern.

In the longer term we would like to have a more elegant search infrastructure, but for the time being this approach seems to work reasonably well.


\subsection{Generating Patterns}
\label{newtemplate}

Figure~\ref{templates} shows some of the patterns that we search for. 

As the figure shows, not all patterns have exactly the same meaning. We divide patterns into three fuzzily-grouped categories:

\begin{itemize}
 \item {\bf False:} The author believes the claim is wrong
 \item {\bf Believed by others:} The author believes others think this, and it is likely that the author disagrees. Further inspection is required to determine whether the author disagrees with the claim. 
 \item {\bf Could be challenged:} The author feels the need to state that someone believes something, which suggests that this is something that one might not believe. For example, if an author says ``I think America is a great place'' this doesn't suggest that the claim is false, but the mere fact that they declared that they thought that suggests that they are aware that the claim could be challenged.
\end{itemize}

We grouped patterns into categories by hand, based on a brief inspection of pages that were using each of the patterns. 

Which patterns one should use depends on what one wishes to use the resulting corpus of disputed claims for. If one wants to only have claims for which we can present web pages that argue that the claim is false, then only the first set of patterns should be used. If however one just wishes to see what things people think are worthy of having opinions expressed about, then the full set is useful.

We started with an initial set of patterns that we wrote by hand. We then found additional templates by searching for the text of known disputed claims, and observing what text commonly occurred as a prefix of a known disputed claim. From this set of candidate prefix patterns, we manually selected patterns that we believed would lead us, as a human, to infer that the author thought the claim was disputed, believed by others, or open to challenge.

\todo{Give some stats about our patterns}
\todo{Use Yahoo BOSS counts rather than Google}
\note{It seems silly to use Google's counts for these when we are using Yahoo BOSS. Someone should write a script to re-generate this table using Yahoo BOSS counts.}

\todo{Analyse how well our date targetting works}

\begin{figure}[tb]
  \begin{tabular}{|ll|}
    \hline
    {\bf Google Count\footnotemark} & {\bf Template}\\ 
    \hline
    \multicolumn{2}{|c|}{\it Something that could be challenged}\\
    163,000,000 & believe that \\
    206,000,000 & think that \\
    49,500,000 & idea that \\
    33,600,000 & claim that \\
   \hline
   \multicolumn{2}{|c|}{\it Something others believe}\\
    120,000,000 & the belief that \\
    49,000,000 & who believe that \\
    50,500,000 & who think that \\
    10,100,000 & believing that \\
    9,430,000 & claiming that \\
    \hline
    \multicolumn{2}{|c|}{\it Something false}\\
    243,000,000 & it is not the case that \\
    30,700,000 & it is not true that \\
    15,600,000 & the misconception that \\
    13,000,000 & the delusion that \\
    12,400,000 & the myth that \\
    9,960,000 & into believing that \\
    8,970,000 & disagree with the claim that \\
    7,190,000 & the mistaken belief that \\
    6,160,000 & disagree with the assertion that \\
    4,950,000 & the fallacy that \\
    3,720,000 & the lie that \\
    3,700,000 & mistaken belief that \\
    3,600,000 & the false belief that \\
    3,440,000 & the deceit that \\
    2,290,000 & cast doubt on the claim that \\
    2,140,000 & the deception that \\
    1,760,000 & the misunderstanding that \\
    1,630,000 & mistakenly believe that \\
    1,090,000 & false claim is that \\
    1,290,000 & the hoax that \\
    698,000 & absurdity of the claim that \\
    681,000 & falsely claimed that \\
    659,000 & no longer think that \\
%     654,000 & urban legend that \\
%     530,000 & the absurd idea that \\
%     373,000 & erroneously believe that \\
%     334,000 & the fabrication that \\
%     103,000 & falsely claiming that \\
%     140,000 & erroneous belief that \\
%     72,600 & falsely claim that \\
%     152,000 & bogus claim that \\
%     224,000 & the fantasy that \\
%     171,000 & incorrectly claim that \\
%     249,000 & incorrectly claimed that \\
%     232,000 & incorrectly believe that \\
%     98,900 & stupidly believe that \\
%     429,000 & falsely believe that \\
%     376,000 & wrongly believe that \\
%     189,000 & falsely suggests that \\
%     280,000 & falsely claims that \\
%     148,000 & false claim that \\
%     136,000 & no longer believe that \\
%     65,300 & urban myth that \\
%     59,500 & falsely stated that \\
    \hline
  \end{tabular}
  \label{templates}
  \caption{Some of the templates that we use to search for disputed claims, and Google's rough estimate of how many times each one appears on the web}
 
\end{figure}


\subsection{Filtering Statements}
\label{filterclaim}

We pass each such chunk of raw text to a natural language parser (custom grammar, and an NLTK~\cite{nltk} regexp parser). We throw away any chunk of text that does not appear to be a statement, or which uses a referent (such as ``it'' or ``he'') that cannot be easily resolved. Figure~\ref{filtered} shows examples of phrases that we filter.

\begin{figure*}[t]
\begin{tabular}{|l|l|l|}
  \hline
  {\bf extracted text} & {\bf accepted} & {\bf reason} \\
  \hline
  {\it the false claim that} won't go away & reject & ``wont go away'' is not a statement \\
  {\it falsely claimed that} he didn't do it & reject & ``he'' and ``it'' are unbound referents \\ 
  {\it wrongly believe that} the moon is made of cheese & accept & \\
  {\it false claim that} Elvis is alive despite all evidence & \\
  \hline
\end{tabular}
\label{filtered}
\caption{We filter out text that does not look like a statement}

\end{figure*}

\todo{Our parser is currently a bit rubbish. Need to work out how to do this properly.}
\todo{We need to use the correct terminology here.}

\x{Remove anything containing stuff like 'if'}
\x{Disallow possesives like its/his etc unless a noun has come first}
\x{Strip HTML tags}


\section{Using Disputed Claims}

\x{We are building lots of applications that will use this data set.}
\x{Dispute Finder highlights disputed claims on the web.}
\x{Tell people when audio they hear is disputed.}
\x{Tell people when a source you trust disagrees with others.}
\x{Tell me when a source I read in the past is now disputed.}
\x{Tell me how contentious a topic is.}
\x{Let me explore what is disputed about a particular topic.}
\x{Tell me when something new is disputed about a topic I care about.}
\x{Examine the zeitgeist of disputes.}
\x{Tell me about things that are disputed by the kind of sites I trust.}
\x{We also want to make this data set available to others.}

\section{Analysing the Data}

\subsection{Data Quality}

\x{Look at a random sample of the data and say how clean it is.}
\x{Evaluate the quality of the claims that we extracted.}
\x{Evaluate the quality of the clusters that we created.}
\x{Evaluate the quality of the different individual templates - which ones have the highest quality claims.}

\subsection{Trends over time}

\x{Named entity recognition}

\x{Look at what noun phrases were most disputed at particular years.}
\x{Look at what noun phrases were most disputed on particular days.}

\x{We remove exact-match strings from the same domain - to avoid republishing of same article.}

\section{Conclusions}

\x{We can use the web as a corpus to determine what people think is disputed.}


%\section{Acknowledgments}

\todo{Do we want to have acknowledgements}
% We would like to Thank Barbara Rosario for help with textual entailment, and 
% Acknowledgements omitted for blind submission. Dispute Finder uses icons from the free FamFamFam Silk\footnote{http://famfamfam.com} collection.

% mendelybib is the bibliograph from our shared mendely space and is auto-generated
% localbib is used for manually adding anything we don't want to add to mendeley, particularly for
% people who are not mendeley users
\bibliography{mendeleybib,localbib}

\end{document}



